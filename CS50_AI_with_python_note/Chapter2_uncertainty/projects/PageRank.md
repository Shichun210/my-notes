## Â∫è

> Ëøô‰∏ÄËØæÁöÑÂÜÖÂÆπ‰∏çÂ•ΩÁêÜËß£ÔºåÊ∂âÂèäÊ¶ÇÁéáÈóÆÈ¢òÔºåÊØîËæÉÊäΩË±°ÔºåÂÖ¨ÂºèÂèàÂ§ö„ÄÇËä±‰∫ÜÂ•Ω‰∫õÊó∂Èó¥Êï¥ÁêÜÁ¨îËÆ∞ÔºåËøòÁÆóÊääÊ†∏ÂøÉÁöÑÁü•ËØÜÁÇπÁªôÊêûÊ∏ÖÊ•ö‰∫ÜÔºå‰ΩÜÊòØÊõ¥Ê∑±ÂÖ•ÁöÑ‰∏Ä‰∫õÁü•ËØÜÁÇπÊ≤°ÊúâÂÆåÂÖ®ÊéåÊè°„ÄÇÊâÄ‰ª•ÂºÄÂßãÂÅöËØæÂêé‰Ωú‰∏öÁöÑÊó∂ÂÄôÊ≤°ÊúâÂ§™Â§ßÁöÑ‰ø°ÂøÉ„ÄÇÊ≤°ÂÖ≥Á≥ªÔºåÂÖàÂéªÂÅöÔºåÈÅáÂà∞‰ªÄ‰πàÈóÆÈ¢òÂÜçÂéªËß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢òÔºå



## È¢òÁõÆË¶ÅÊ±Ç

ÂÆûÁé∞‰∏Ä‰∏™Á±ª‰ººGoogleÊó©ÊúüÊêúÁ¥¢ÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÔºåÁî®‰∫éÂà§Êñ≠ÁΩëÈ°µÁöÑÈáçË¶ÅÊÄß„ÄÇÊüê‰∏™ÁΩëÈ°µÁöÑÈáçË¶ÅÊÄß‰∏ç‰ªÖÂèñÂÜ≥‰∫éËÆøÈóÆÂÆÉÁöÑ‰∫∫Êï∞ÔºåËøòÂèñÂÜ≥‰∫éÈìæÊé•Âà∞ÂÆÉÁöÑÁΩëÈ°µÁöÑÈáçË¶ÅÊÄß„ÄÇ

- ÁõÆÊ†áÔºöËÆ°ÁÆó‰∏ÄÁªÑÁΩëÈ°µÁöÑpagerankÂÄºÔºåËøô‰∏™ÂÄºÂèØ‰ª•Ë°°ÈáèÁΩëÈ°µÁöÑÈáçË¶ÅÊÄß„ÄÇ
- ËæìÂÖ•Ôºö‰∏Ä‰∫õÁΩëÈ°µÁöÑÈõÜÂêàÔºåÊØè‰∏™ÁΩëÈ°µÈáåÈù¢ÂèØËÉΩÊúâÂá∫ÈìæÊé•Âà∞ÂÖ∂‰ªñÁΩëÈ°µÔºå‰πüÂèØËÉΩÊ≤°Êúâ
- ËæìÂá∫ÔºöÊØè‰∏™ÁΩëÈ°µÁöÑpagerankÂÄºÔºåÊù•Ë°®Á§∫‰∏Ä‰∏™**ÈöèÊú∫**ËÆøÈóÆËÄÖÊµèËßàÂà∞ËØ•ÁΩëÈ°µÁöÑÊ¶ÇÁéá



## È¢òÁõÆÂàÜÊûê

Êó¢ÁÑ∂ÊúâÁöÑÁΩëÈ°µÊ≤°ÊúâÂá∫ÈìæÔºå‰∏∫‰∫ÜÁ°Æ‰øùÊàë‰ª¨ÂßãÁªàËÉΩÂ§üËÆøÈóÆÁΩëÈ°µÈõÜÂêà‰∏≠ÁöÑÂÖ∂‰ªñÈ°µÈù¢ÔºåÊàë‰ª¨Â∞ÜÂú®Ê®°Âûã‰∏≠ÂºïÂÖ•‰∏Ä‰∏™ÈòªÂ∞ºÂõ†Â≠ê`d`„ÄÇÈöèÊú∫ÊµèËßàËÄÖÊúâÊ¶ÇÁéá`d`Ôºà`d`ÈÄöÂ∏∏ËÆæÁΩÆ‰∏∫ Â∑¶Âè≥`0.85`Ôºâ‰ºö‰ªéÂΩìÂâçÈ°µÈù¢ÁöÑÂá∫Èìæ‰∏≠ÈöèÊú∫ÈÄâÊã©‰∏Ä‰∏™„ÄÇ‰ΩÜÈô§Ê≠§‰πãÂ§ñÔºàÊ¶ÇÁéá‰∏∫`1 - d`ÔºâÔºåÈöèÊú∫ÊµèËßàËÄÖ‰ºö‰ªéÁΩëÈ°µÂ∫ìÁöÑÊâÄÊúâÈ°µÈù¢ÔºàÂåÖÊã¨‰ªñ‰ª¨ÂΩìÂâçÊâÄÂú®ÁöÑÈ°µÈù¢Ôºâ‰∏≠ÈöèÊú∫ÈÄâÊã©‰∏Ä‰∏™„ÄÇ

> **ÈöèÊú∫ÊµèËßàËÄÖÊ®°Âûã**ÔºàRandom Surfer ModelÔºâÔºö
>
> - ÊµèËßàËÄÖÈöèÊú∫ÁÇπÂáªÁΩëÈ°µ‰∏äÁöÑÈìæÊé•„ÄÇ
> - ÊúâÊ¶ÇÁéá `damping_factor` ÊåâÁΩëÈ°µÂá∫ÈìæÊé•Ë∑≥ËΩ¨ÔºåÊØè‰∏™Âá∫ÈìæÂπ≥ÂùáÂàÜÈÖç`damping_factor`„ÄÇ
> - ÊúâÊ¶ÇÁéá `1 - damping_factor` ÈöèÊú∫Ë∑≥ËΩ¨Âà∞‰ªª‰ΩïÁΩëÈ°µ„ÄÇ



ÈúÄË¶Å‰ΩøÁî®ÊäΩÊ†∑Ê≥ïÂíåËø≠‰ª£ÂÖ¨ÂºèÊ≥ïÔºåËÆ°ÁÆóÂá∫ÊØè‰∏™È°µÈù¢ÁöÑpagerank



### ËΩ¨Êç¢Ê®°Âûã

transition_modeÂáΩÊï∞Â∫îËØ•ËøîÂõû‰∏Ä‰∏™Â≠óÂÖ∏ÔºåËØ•Â≠óÂÖ∏Ë°®Á§∫ÁªôÂÆöÈ°µÈù¢ÈõÜ„ÄÅÂΩìÂâçÈ°µÈù¢ÂíåÈòªÂ∞ºÂõ†Â≠êÁöÑÊÉÖÂÜµ‰∏ãÈöèÊú∫ÊµèËßàËÄÖÊé•‰∏ãÊù•ËÆøÈóÆÂì™‰∏™È°µÈù¢ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ

ÁÆóÊ≥ïÂèÇÁÖß‰∏äËø∞‚ÄúÈöèÊú∫ÊµèËßàËÄÖÊ®°Âûã‚Äù



### üëâÊäΩÊ†∑Ê≥ï

> sample_pagerankÂáΩÊï∞Êé•Êî∂3‰∏™ÂèÇÊï∞ÔºåÁΩëÈ°µËØ≠ÊñôÂ∫ì`corpus`, ÈòªÂ∞ºÂõ†Â≠ê`damping_factor`Âíå‰ª•ÂèäÊ†∑Êú¨Êï∞Èáè`n`„ÄÇ

**ÂÖ¨Âºè**Ôºö pagerank = P(page) = Âú®Ê†∑Êú¨‰∏≠ËÆøÈóÆËØ•È°µÈù¢ÁöÑÊ¨°Êï∞/ÊÄªÊ†∑Êú¨Êï∞

**ÊÄùË∑Ø**Ôºö

1. ÈöèÊú∫ÈÄâÊã©‰∏Ä‰∏™ÁΩëÈ°µ‰Ωú‰∏∫ÂàùÂßãËÆøÈóÆÈ°µÈù¢„ÄÇ
2. Ê†πÊçÆtransition_modelÂáΩÊï∞Â§öÊ¨°ÊäΩÂèñÊ†∑Êú¨ÔºåËé∑ÂèñÂà∞‰∏ã‰∏Ä‰∏™ËÆøÈóÆÁöÑÈ°µÈù¢Ê¶ÇÁéáÂàÜÂ∏É
3. Êàë‰ª¨ÂÜçÊää‰∏ã‰∏Ä‰∏™ËÆøÈóÆÁöÑÈ°µÈù¢Ê¶ÇÁéáÂàÜÂ∏É‰Ωú‰∏∫ÊùÉÈáçÔºåÈöèÊú∫ÈÄâÊã©‰∏Ä‰∏™È°µÈù¢ËÆøÈóÆ
4. Á¥ØÂä†Ë¢´ËÆøÈóÆÈ°µÈù¢ÁöÑcounter
5. Ë∑≥ËΩ¨Âà∞2ÁªßÁª≠Âæ™ÁéØÔºåÁõ¥Ëá≥ËææÂà∞Ê†∑Êú¨ÊäΩÂèñÊï∞Èáè
6. ÊúÄÁªàÊ†∑Êú¨ÊäΩÂèñÂÆåÊØïÔºåÁªüËÆ°ÊØè‰∏™È°µÈù¢ÁöÑrank



```mermaid
flowchart TD
    A[ÈöèÊú∫ÈÄâÊã©Ëµ∑ÂßãÈ°µÈù¢ InitPage] --> B[Âæ™ÁéØ n Ê¨°ÈááÊ†∑]
    B --> C[Ë∞ÉÁî® transition_model ÁîüÊàêÊ¶ÇÁéáÂàÜÂ∏É]
    C --> D[Ê†πÊçÆÊ¶ÇÁéáÂàÜÂ∏ÉÈÄâÊã©‰∏ã‰∏ÄÈ°µÂπ∂Á¥ØÂä†ËÆ°Êï∞]
    D --> B
    B --> E[ËÆ°ÁÆó PageRank = ËÆøÈóÆÊ¨°Êï∞ / n]
    E --> F[ËøîÂõû pageRank Â≠óÂÖ∏]
```





### üëâËø≠‰ª£ÂÖ¨ÂºèÊ≥ï

> `iterate_pagerank`ÂáΩÊï∞Â∫îÊé•Âèó‰∏Ä‰∏™ÁΩëÈ°µËØ≠ÊñôÂ∫ì`corpus`Âíå‰∏Ä‰∏™ÈòªÂ∞ºÂõ†Â≠ê`damping_factor`ÔºåÊ†πÊçÆËø≠‰ª£ÂÖ¨ÂºèËÆ°ÁÆó PageRankÔºåÁ≤æÁ°ÆÂà∞ÊØè‰∏™È°µÈù¢ÁöÑ PageRank ËØØÂ∑ÆÂ∞è‰∫é`0.001`ÂêéËøîÂõû„ÄÇ

**ÂÖ¨ÂºèÔºö**
$$
PR(p) = \frac{1-d}{N} + d \sum_{i \in M(p)} \frac{PR(i)}{NumLinks(i)}
$$

- $d$ = damping factor
- $N$ = È°µÈù¢ÊÄªÊï∞
- $M(p)$ = ÊâÄÊúâÊåáÂêë $p$ ÁöÑÈ°µÈù¢ÈõÜÂêà
- $NumLinks(i)$ = È°µÈù¢ $i$ ÁöÑÂá∫ÈìæÊï∞Èáè

> [!caution]
>
> ÂØπ‰∫éÊ≤°ÊúâÂá∫ÈìæÁöÑÈ°µÈù¢ÔºåÂÅáËÆæÂÆÉÈìæÊé•Âà∞ÊâÄÊúâÈ°µÈù¢ÔºåÂåÖÊã¨Ëá™Â∑±„ÄÇ



**ÁÆóÊ≥ïÂàÜÊûêÔºö**

ÂØπ‰∫éÈ°µÈù¢ $p$ÔºåPageRank $PR(p)$ Ë°®Á§∫ÈöèÊú∫ÊµèËßàËÄÖÊúÄÁªàÂÅúÁïôÂú® $p$ ÁöÑÊ¶ÇÁéá„ÄÇÈöèÊú∫ÊµèËßàËÄÖÊúâ‰∏§ÁßçÊñπÂºèÂà∞ËææÈ°µÈù¢ $p$Ôºö

1. ÈöèÊú∫Ë∑≥ËΩ¨Âà∞È°µÈù¢$p$
   - Ê¶ÇÁéá‰∏∫`1- d`
   - Á≠âÊ¶ÇÁéáÂàÜÈÖçÂà∞ÊâÄÊúâÁöÑÈ°µÈù¢:$$\frac{1-d}{N}$$
2. ÈÄöËøáÈìæÊé•‰ªéÂÖ∂‰ªñÈ°µÈù¢Ë∑≥ËΩ¨Âà∞$p$
   - Ë∑≥ËΩ¨Ê¶ÇÁéá‰∏∫$d$
   - ÂØπ‰∫éÊØè‰∏™ËøûÊé•Âà∞$p$ÁöÑÈ°µÈù¢$i$Ôºö
     - È°µÈù¢$i$ÁöÑpagerank‰∏∫PR($i$)
     - È°µÈù¢$i$Êúâ$NumLinks(i)$‰∏™Âá∫Èìæ
     - ‰ªé $i$ Âà∞ $p$ ÁöÑÊ¶ÇÁéá = $\frac{PR(i)}{NumLinks(i)}$

```mermaid
flowchart TD
    A["Initialize PageRank = 1 / N"] --> B["Iteratively update each page's PageRank"]
    B --> C["Compute new PR: PR(p) = (1-d)/N + d * sum(PR(q)/L(q))"]
    C --> D{"Converged?"}
    D -- "Yes, abs(newRank - pageRank) < delta" --> E["Return newRank"]
    D -- "No" --> F["pageRank = newRank"]
    F --> B
```



---

## ‰ª£Á†ÅÂÆûÁé∞

```python
import os
import random
import re
import sys

DAMPING = 0.85
SAMPLES = 10000


def main():
    if len(sys.argv) != 2:
        sys.exit("Usage: python pagerank.py corpus")
    corpus = crawl(sys.argv[1])
    ranks = sample_pagerank(corpus, DAMPING, SAMPLES)
    print(f"PageRank Results from Sampling (n = {SAMPLES})")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")
    ranks = iterate_pagerank(corpus, DAMPING)
    print(f"PageRank Results from Iteration")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")


def crawl(directory):
    """
    Parse a directory of HTML pages and check for links to other pages.
    Return a dictionary where each key is a page, and values are
    a list of all other pages in the corpus that are linked to by the page.
    """
    pages = dict()

    # Extract all links from HTML files
    for filename in os.listdir(directory):
        if not filename.endswith(".html"):
            continue
        with open(os.path.join(directory, filename)) as f:
            contents = f.read()
            links = re.findall(r"<a\s+(?:[^>]*?)href=\"([^\"]*)\"", contents)
            pages[filename] = set(links) - {filename}

    # Only include links to other pages in the corpus
    for filename in pages:
        pages[filename] = set(
            link for link in pages[filename]
            if link in pages
        )

    return pages


def transition_model(corpus, page, damping_factor):
    """
    Return a probability distribution over which page to visit next,
    given a current page.

    With probability `damping_factor`, choose a link at random
    linked to by `page`. With probability `1 - damping_factor`, choose
    a link at random chosen from all pages in the corpus.
    """
    prob_dist = {}

    # Initialize probability
    for a in corpus:
        prob_dist[a] = (1 - damping_factor) / len(corpus)

    # Probability is evenly distributed among all outgoing pages.
    if corpus[page]:
        for p in corpus[page]:
            prob_dist[p] += damping_factor / len(corpus[page])

    # No outgoing links from the page, pretend it has links to all pages
    else:
        for p in corpus:
            prob_dist[p] += damping_factor / len(corpus)

    return prob_dist


def sample_pagerank(corpus, damping_factor, n):
    """
    Return PageRank values for each page by sampling `n` pages
    according to transition model, starting with a page at random.

    Return a dictionary where keys are page names, and values are
    their estimated PageRank value (a value between 0 and 1). All
    PageRank values should sum to 1.
    """
    # Pick a random page to start
    InitPage = random.choice(list(corpus.keys()))
    page = InitPage
    pageCount = {}
    pageRank = {}

    # Initialize each page's Rank to 0
    for p in corpus:
        pageRank[p] = 0

    for i in range(n):
        # Generate a sample from transition model
        sample = transition_model(corpus, page, damping_factor)
        # Randomly choose a page based on the sample's possibility distribution
        possiblePage = random.choices(list(sample.keys()), list(sample.values()))[0]

        # Accumulate the count of pages visited
        if possiblePage not in pageCount:
            pageCount[possiblePage] = 1
        else:
            pageCount[possiblePage] += 1
        # Move to next page
        page = possiblePage

    # Calaculate page rank according to the visited counter
    for p in pageCount:
        pageRank[p] = pageCount[p] / n
    
    return pageRank


def iterate_pagerank(corpus, damping_factor):
    """
    Return PageRank values for each page by iteratively updating
    PageRank values until convergence.

    Return a dictionary where keys are page names, and values are
    their estimated PageRank value (a value between 0 and 1). All
    PageRank values should sum to 1.
    """
    pageRank = {}
    delta = 0.001

    # Initialize each page rank evenly
    for p in corpus:
        pageRank[p] = 1 / len(corpus)
    
    # PR(p)=(1 ‚àí damping_factor)/n + damping_factor * q‚ààM(p)‚àëPR(q)/ L(q)
    """e.g. There are 3 pages: A, B, C
    {
     "A.html": {"B.html", "C.html"},
     "B.html": {"C.html"},
     "C.html": {"A.html"}
    }

    PR(A) = ((1 - damping_factor ) / 3) + damping_factor * ((PR(B) / L(B)) + (PR(C) / L(C)))

    L(q) is the number of outgoing links from page q.

    """
    while True:
        newRank = {}
        for p in corpus:
            newRank[p] = (1 - damping_factor) / len(corpus)
            for links in corpus:
                # if a page has no outgoing links, pretend it has links to all pages including itself
                if not corpus[links]:
                    newRank[p] += damping_factor * pageRank[links] / len(corpus)
                # if a page links to p, add rank as the formula
                else:
                    if p in corpus[links]:
                        newRank[p] += damping_factor * pageRank[links] / len(corpus[links])
        
        # If the delta less than threshold 0.001 for all pages, we can stop the loop and return the rank
        if all(abs(newRank[k] - pageRank[k]) < delta for k in newRank):
            return newRank

        pageRank = newRank.copy()


if __name__ == "__main__":
    main()

```



## ËØæÂêéÊÄùËÄÉ

ËôΩÁÑ∂ËØæÁ®ãÂÜÖÂÆπÊ∂âÂèäÁöÑÊ¶ÇÁéáËÆ∫ÂíåÂÖ¨ÂºèÊé®ÂØºÊØîËæÉÊäΩË±°ÔºåÂàöÂºÄÂßãÁêÜËß£Êó∂Êúâ‰∫õÂêÉÂäõÔºå‰ΩÜÁúüÊ≠£ËøõÂÖ•‰ª£Á†ÅÂÆûÁé∞ÁéØËäÇÊó∂ÔºåÂèëÁé∞ÈöæÂ∫¶Âπ∂‰∏çÁÆóÈ´ò„ÄÇ‰Ωú‰∏öÁöÑÂÖ≥ÈîÆÂú®‰∫é**‰ªîÁªÜÈòÖËØªÈ¢òÁõÆË¶ÅÊ±Ç**ÔºåÁÑ∂ÂêéÊ†πÊçÆÂÖ¨Âºè‰∏ÄÊ≠•Ê≠•ÂéªÂÆûÁé∞Âç≥ÂèØ„ÄÇÁõ∏ÊØîÁºñÁ®ãÔºåÁêÜËß£È¢òÁõÆÈáåÁöÑÊ®°ÂûãÂÅáËÆæÂíåËÆ°ÁÆóÈÄªËæëÂèçËÄåÊõ¥Èöæ„ÄÇÂ¶ÇÊûú‰∏ÄÂºÄÂßãÊ≤°ÂÆåÂÖ®ËØªÊáÇË¶ÅÊ±ÇÔºåÂæàÂÆπÊòìËµ∞ÂÅè„ÄÇ

